# -*- coding: utf-8 -*-
"""Price Prediction Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yuySS6g-WB5y2ZM2ym1kxpKfykySxQf7
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.impute import SimpleImputer

# --- LOAD DATA ---
df = pd.read_csv('uae_real_estate_2024.csv')

# --- PREPROCESSING ---

# Convert price to numeric, handle missing or malformed values
def clean_price(price):
    try:
        price = str(price).replace(',', '').replace('AED', '').strip()
        return float(re.sub('[^0-9.]', '', price))
    except:
        return np.nan

df['price'] = df['price'].apply(clean_price)
df = df.dropna(subset=['price'])

# Filter only 'Residential for Sale' listings (if applicable)
df = df[df['type'] == 'Residential for Sale']

# Feature extraction and cleanup
df['size_sqft'] = df['sizeMin'].str.extract('(\d+)').astype(float)

# Fill missing columns with median or mode
for col in ['bathrooms', 'bedrooms', 'size_sqft']:
    df[col] = pd.to_numeric(df[col], errors='coerce')
    median_val = df[col].median()
    df[col].fillna(median_val, inplace=True)

# Simplify furnishing status
df['furnishing'] = df['furnishing'].fillna('No').map({'YES': 'Yes', 'NO': 'No', 'Yes': 'Yes', 'No': 'No'})

# Select relevant features for model
features = ['bathrooms', 'bedrooms', 'size_sqft', 'furnishing']

X = df[features]
y = df['price']

# Column types for pipeline
numeric_features = ['bathrooms', 'bedrooms', 'size_sqft']
categorical_features = ['furnishing']

# Preprocessing pipeline
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# --- MODELING ---

# Model options: Ridge for regularization and RandomForest for nonlinear
models = {
    'Ridge': Ridge(),
    'RandomForest': RandomForestRegressor(random_state=42)
}

for name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('regressor', model)])
    pipeline.fit(X, y)
    y_pred = pipeline.predict(X)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    r2 = r2_score(y, y_pred)
    print(f'{name} - RMSE: {rmse:.2f}, R2: {r2:.4f}')

# --- CROSS-VALIDATION ---
ridge_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                 ('regressor', Ridge(alpha=1.0))])

cv_scores = cross_val_score(ridge_pipeline, X, y, cv=5, scoring='neg_root_mean_squared_error')
print(f'5-fold CV RMSE (Ridge): {-np.mean(cv_scores):.2f}')

# --- Feature importance (Random Forest) ---
rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('regressor', RandomForestRegressor(random_state=42))])
rf_pipeline.fit(X, y)

# Extract feature importance after onehot encoding
feature_names = numeric_features + list(rf_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out())
importances = rf_pipeline.named_steps['regressor'].feature_importances_

fi_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=False)
print(fi_df)

# Plot feature importance
plt.figure(figsize=(8, 4))
sns.barplot(x='importance', y='feature', data=fi_df)
plt.title('Feature Importance - Random Forest')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Collect metrics for models
metrics = []
for name, model in models.items():
    y_pred = pipeline.predict(X)  # Make predictions on full dataset for simplicity
    rmse = mean_squared_error(y, y_pred) ** 0.5
    mae = mean_absolute_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    metrics.append({'Model': name, 'RMSE': rmse, 'MAE': mae, 'R2': r2})

# Create comparison dataframe
compare_df = pd.DataFrame(metrics)
print("Model Comparison:\n", compare_df)

# Plot metrics
compare_df.set_index('Model')[['RMSE', 'MAE']].plot(kind='bar', figsize=(10,5))
plt.title('Error Metrics Comparison')
plt.ylabel('Error')
plt.show()

# Residuals plot for each model
for name, model in models.items():
    y_pred = pipeline.predict(X)
    residuals = y - y_pred
    sns.histplot(residuals, kde=True)
    plt.title(f'Residual Distribution - {name}')
    plt.xlabel('Residual')
    plt.show()

# Feature importance for Random Forest only (example)
importances = rf_pipeline.named_steps['regressor'].feature_importances_
feature_names = numeric_features + list(rf_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out())
feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)

sns.barplot(x='Importance', y='Feature', data=feat_imp_df)
plt.title('Feature Importance - Random Forest')
plt.show()